{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\namsik\\anaconda3\\lib\\site-packages (2.4.6)\n",
      "Requirement already satisfied: py4j==0.10.7 in c:\\users\\namsik\\anaconda3\\lib\\site-packages (from pyspark) (0.10.7)\n",
      "Requirement already satisfied: findspark in c:\\users\\namsik\\anaconda3\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "try:\n",
    "    findspark.init()\n",
    "except:\n",
    "    print('아래 방법으로 path 재설정')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "자바 설치\n",
    "- https://java.com/ko/download/\n",
    "spark 설치\n",
    "- http://spark.apache.org/downloads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7\n",
      "C:\\Program Files\\Java\\jdk1.8.0_231\n",
      "C:\\Users\\Namsik\\Anaconda3;C:\\Users\\Namsik\\Anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\Namsik\\Anaconda3\\Library\\usr\\bin;C:\\Users\\Namsik\\Anaconda3\\Library\\bin;C:\\Users\\Namsik\\Anaconda3\\Scripts;C:\\Users\\Namsik\\Anaconda3\\bin;C:\\Users\\Namsik\\Anaconda3\\condabin;C:\\Program Files\\Rockwell Software\\RSCommon;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;C:\\windows\\system32;C:\\windows;C:\\windows\\System32\\Wbem;C:\\windows\\System32\\WindowsPowerShell\\v1.0;C:\\windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.4.0;\";C:\\Users\\Namsik\\AppData\\Local\\Microsoft\\WindowsApps\\\";C:\\Program Files\\Git\\cmd;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Users\\Namsik\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Windows\\system32;C:\\Windows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(os.environ['SPARK_HOME'])\n",
    "except:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7'\n",
    "    print(os.environ['SPARK_HOME'])\n",
    "try:\n",
    "    print(os.environ['JAVA_HOME'])\n",
    "except KeyError:\n",
    "    os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "    print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark_data' in os.listdir():pass\n",
    "else:\n",
    "    import wget\n",
    "    import zipfile\n",
    "    wget.download('https://www.dropbox.com/sh/5c9daz43f5lstzd/AAAPN_iMkKzgqJ1k3tml7FNHa?dl=1')\n",
    "    with zipfile.ZipFile('spark_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('spark_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "스파크 애플리케이션은 SparkSession이라 불리는 드라이버 프로세스로 제어합니다. \n",
    "SparkSession 인스턴스는 사용자가 정의한 처리 명령을 클러스터에서 실행합니다. \n",
    "하나의 SparkSession은 하나의 스파크 애플리케이션에 대응합니다. \n",
    "스칼라와 파이썬 콘솔을 시작하면 spark변수로 SparkSession을 사용할 수 있습니다.\n",
    "\"\"\"\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "일정범위의 숫자를 만드는 작업. \n",
    "이 숫자들은 스프레드시트에서 컬럼명을 지정한것과 같음.\n",
    "\"\"\"\n",
    "myRange = spark.range(1000).toDF('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "방금 생성한 DataFrame(MyRange)는 한개의 컬럼과 1000개의 로우로 구성되며 각 로우에는 0부터999까지의 값이 할당됨.\n",
    "이 숫자들은 분산 컬렉션을 나타내며 클러스터 모드에서 코드예제를 실행하면 숫자 범위의 각 부분이 서로 다른 익스큐터에 할당됨.\n",
    "이것이 스파크의 DataFrame임.\n",
    "\"\"\"\n",
    "myRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(number=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0),\n",
       " Row(number=1),\n",
       " Row(number=2),\n",
       " Row(number=3),\n",
       " Row(number=4),\n",
       " Row(number=5),\n",
       " Row(number=6),\n",
       " Row(number=7),\n",
       " Row(number=8),\n",
       " Row(number=9)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "스파크의 핵심 데이터 구조는 불변성을 가짐. \n",
    "한번 생성하면 변경할 수 없으며 변경을 하려면 변경 방법을 스파크에 알려줘야함.\n",
    "이때 사용하는 명령을 트렌스포메이션이라 하며 아래 코드는 짝수를 찾는 간단한 트랜스포메이션 예제\n",
    "\"\"\"\n",
    "divisBy2 = myRange.where('number % 2 = 0')\n",
    "divisBy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(number=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0),\n",
       " Row(number=2),\n",
       " Row(number=4),\n",
       " Row(number=6),\n",
       " Row(number=8),\n",
       " Row(number=10),\n",
       " Row(number=12),\n",
       " Row(number=14),\n",
       " Row(number=16),\n",
       " Row(number=18)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count(),myRange.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "스파크는 다양한 데이터소스를 지원함.\n",
    "아래 예제에서는 스파크 DataFrame의 스키마 정보를 알아내는 스키마 추론기능을 사용함.\n",
    "(스키마 = 데이터의 컬럼 및 컬럼의 데이터 유형에 대한 정보)\n",
    "그리고 파일의 첫 로우를 헤더로 지정하는 옵션도 설정\n",
    "\"\"\"\n",
    "flightData2015 = spark.read.option('inferSchema','true').option('header','true')\\\n",
    ".csv('spark_data/flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "take 액션을 호출하면 head와 같은 결과를 확인할 수 있음\n",
    "해당 DataFrame은 ['DEST_COUNTRY_NAME','ORIGHIN_COUNTRY_NAME','count']라는 컬럼을 가짐\n",
    "\"\"\"\n",
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#238 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#238 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#236,ORIGIN_COUNTRY_NAME#237,count#238] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Dropbox/spark_data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "정수 데이터 타입인 count 컬럼을 기준으로 explain액션을 이용해 데이터를 정렬.\n",
    "sort는 호출시 데이터에 아무런 변화도 일어나지않음.\n",
    "아래 호출된 내용은 실행계획으로 디버깅과 스파크의 실행과정을 이해하는데 도움을 주는 도구일 뿐임\n",
    "\"\"\"\n",
    "flightData2015.sort('count').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "앞서 말한것과같이 sort를 한다해도 바뀌진않고 실행계획을 실행하여야함\n",
    "\"\"\"\n",
    "flightData2015.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "트랜스포메이션 실행 계획을 시작하기위해선 액션을 호출해야하며 이를 위해 몇가지 설정이 필요함.\n",
    "스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션을 생성하며 이 값을 5로 설정해 셔플의 출력 파티션 수를 줄임\n",
    "\"\"\"\n",
    "spark.conf.set('spark.sql.shuffle.partitions','5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort('count').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "스파크는 언어에 상관없이 같은 방식으로 트랜스포메이션을 실행할 수 있음.\n",
    "사용자가 SQL이나 DataFrame으로 비즈니스 로직을 표현하면 스파크에서 실제 코드를 실행하기 전에 그 로직을 기본 실행 계획으로 컴파일함.\n",
    "스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰로 등록한 후 SQL 쿼리를 사용할 수 있음.\n",
    "아래 코드로 createOrReplaceTempView 메서드를 호출하면 모든 DataFrame을 테이블이나 뷰로 만들 수 있음\n",
    "\"\"\"\n",
    "flightData2015.createOrReplaceTempView('flight_data_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#236], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#236, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#236], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#236] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Dropbox/spark_data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#236], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#236, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#236], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#236] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Dropbox/spark_data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이제 SQL로 데이터를 조회할 수 있으며 spark.sql 메서드로 SQL 쿼리를 실행함\n",
    "DataFrame에 쿼리를 수행하면 새로운 DataFrame을 반환함.\n",
    "로직을 작성할 때 반복적인 느낌이 들지만 효율적인 방식임\n",
    "아래처럼 sql문으로 수행할 수도있고, DataFrame의 내장함수로도 가능\n",
    "\n",
    "아래 코드는 DataFrame에서 DEST_COUNTRY_NAME 컬럼의 나라 별 로우 갯수를 출력\n",
    "(출발 국가별 로우 갯수)\n",
    "\"\"\"\n",
    "\n",
    "sqlWay = spark.sql(\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\")\n",
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "\n",
    "sqlWay.explain()\n",
    "print('-'*100)\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Moldova', count(1)=1),\n",
       " Row(DEST_COUNTRY_NAME='Bolivia', count(1)=1),\n",
       " Row(DEST_COUNTRY_NAME='Algeria', count(1)=1),\n",
       " Row(DEST_COUNTRY_NAME='Turks and Caicos Islands', count(1)=1),\n",
       " Row(DEST_COUNTRY_NAME='Pakistan', count(1)=1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlWay.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Moldova', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Bolivia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Algeria', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Turks and Caicos Islands', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Pakistan', count=1)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrameWay.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 132)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlWay.count(),dataFrameWay.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN_COUNTRY_NAME</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEST_COUNTRY_NAME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Algeria</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Angola</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Anguilla</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Argentina</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ORIGIN_COUNTRY_NAME  count\n",
       "DEST_COUNTRY_NAME                              \n",
       "Algeria                                1      1\n",
       "Angola                                 1      1\n",
       "Anguilla                               1      1\n",
       "Antigua and Barbuda                    1      1\n",
       "Argentina                              1      1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "번외. pandas 문법으로 시도\n",
    "\n",
    "똑같음\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv('spark_data/flight-data/csv/2015-summary.csv')\n",
    "pandasWay = df.groupby('DEST_COUNTRY_NAME').count()\n",
    "pandasWay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(max(count)=370002)]\n",
      "[Row(max(count)=370002)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "스파크는 빅데이터 문제를 빠르게 해결하는데 필요한 수백개의 함수를 제공.\n",
    "다음 예제에서는 특정 위치를 왕래하는 최대 비행횟수를 구하기 위해 max함수를 사용.\n",
    "max 함수는 DataFrame의 특정 컬럼값을 스캔하면서 이전 최댓값보다 더 큰 값을 찾음.\n",
    "max 함수는 필터링을 수행해 단일 로우를 결과로 반환하는 트랜스포메이션임.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "print(flightData2015.select(max('count')).take(1))\n",
    "print(flightData2015.select(max('count')).take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|DEST_COUNTRY_NAME|destinaton_total|\n",
      "+-----------------+----------------+\n",
      "|    United States|          411352|\n",
      "|           Canada|            8399|\n",
      "|           Mexico|            7140|\n",
      "|   United Kingdom|            2025|\n",
      "|            Japan|            1548|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "위의 코드처럼 max는 단일 로우를 반환하기에 take에 무슨 숫자를 넣든 같은 단일 로우를 반환.\n",
    "이번엔 상위 5개의 도착 국가를 찾아내는 복잡한 코드를 실행.\n",
    "\"\"\"\n",
    "\n",
    "maxSql = spark.sql(\"SELECT DEST_COUNTRY_NAME, sum(count) as destinaton_total\\\n",
    " FROM flight_data_2015\\\n",
    " GROUP BY DEST_COUNTRY_NAME\\\n",
    " ORDER BY sum(count) DESC\\\n",
    " LIMIT 5\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이번엔 sql구문이 아닌 DataFrame 구문\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015.groupBy('DEST_COUNTRY_NAME').sum('count')\\\n",
    ".withColumnRenamed('sum(count)','destination_total')\\\n",
    ".sort(desc('destination_total'))\\\n",
    ".limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   DEST_COUNTRY_NAME|sum(count)|\n",
      "+--------------------+----------+\n",
      "|             Moldova|         1|\n",
      "|             Bolivia|        30|\n",
      "|             Algeria|         4|\n",
      "|Turks and Caicos ...|       230|\n",
      "|            Pakistan|        12|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "==================================================\n",
      "+--------------------+-----------------+\n",
      "|   DEST_COUNTRY_NAME|destination_total|\n",
      "+--------------------+-----------------+\n",
      "|             Moldova|                1|\n",
      "|             Bolivia|               30|\n",
      "|             Algeria|                4|\n",
      "|Turks and Caicos ...|              230|\n",
      "|            Pakistan|               12|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "==================================================\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "==================================================\n",
      "limit(5)와 show(5)는 출력상으로는 같은 의미이지만 limit(5)는 dataframe자체를 변환\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "번외. 위의 코드를 뜯어보자\n",
    "\"\"\"\n",
    "first_action = flightData2015.groupBy('DEST_COUNTRY_NAME').sum('count')\n",
    "first_action.show(5)\n",
    "print('='*50)\n",
    "second_action = first_action.withColumnRenamed('sum(count)','destination_total')\n",
    "second_action.show(5)\n",
    "print('='*50)\n",
    "third_action = second_action.sort(desc('destination_total'))\n",
    "third_action.show(5)\n",
    "print('='*50)\n",
    "print('limit(5)와 show(5)는 출력상으로는 같은 의미이지만 limit(5)는 dataframe자체를 변환')\n",
    "fourth_action = third_action.limit(5)\n",
    "fourth_action.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
