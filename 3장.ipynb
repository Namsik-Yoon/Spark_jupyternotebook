{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.144300\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "스파크를 사용하면 빅데이터 프로그램을 쉽게 개발할 수 있음.\n",
    "spark-submit 명령을 사용해 대화형 셸에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환할 수 있음.\n",
    "사용자는 스파크가 지원하는 프로그래밍 언어로 애플리케이션을 개발한 다음 실행 할 수 있음.\n",
    "가장 간단한 방법은 로컬 머신에서 애플리케이션을 실행하는것으로, 아래는 스파크와 함께 제공되는 스칼라 애플리케이션 예제를 실행하는 예제이다.\n",
    "\n",
    "파이값을 특정 자릿수까지 계산하는 애플리케이션으로 매직커맨드로 실행\n",
    "\"\"\"\n",
    "%run C:/Users/Namsik/Anaconda3/Lib/site-packages/pyspark/examples/src/main/python/pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# %load C:/Users/Namsik/Anaconda3/Lib/site-packages/pyspark/examples/src/main/python/pi.py\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "    spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7\n",
      "C:\\Program Files\\Java\\jdk1.8.0_231\n",
      "C:\\Users\\Namsik\\Anaconda3;C:\\Users\\Namsik\\Anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\Namsik\\Anaconda3\\Library\\usr\\bin;C:\\Users\\Namsik\\Anaconda3\\Library\\bin;C:\\Users\\Namsik\\Anaconda3\\Scripts;C:\\Users\\Namsik\\Anaconda3\\bin;C:\\Users\\Namsik\\Anaconda3\\condabin;C:\\Program Files\\Rockwell Software\\RSCommon;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;C:\\windows\\system32;C:\\windows;C:\\windows\\System32\\Wbem;C:\\windows\\System32\\WindowsPowerShell\\v1.0;C:\\windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.4.0;\";C:\\Users\\Namsik\\AppData\\Local\\Microsoft\\WindowsApps\\\";C:\\Program Files\\Git\\cmd;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Users\\Namsik\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Windows\\system32;C:\\Windows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    findspark.init()\n",
    "except:\n",
    "    try:\n",
    "        print(os.environ['SPARK_HOME'])\n",
    "    except:\n",
    "        os.environ['SPARK_HOME'] = 'C:\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7'\n",
    "        print(os.environ['SPARK_HOME'])\n",
    "    try:\n",
    "        print(os.environ['JAVA_HOME'])\n",
    "    except KeyError:\n",
    "        os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "        print(os.environ['JAVA_HOME'])\n",
    "    print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark_data' in os.listdir():pass\n",
    "else:\n",
    "    import wget\n",
    "    import zipfile\n",
    "    wget.download('https://www.dropbox.com/sh/5c9daz43f5lstzd/AAAPN_iMkKzgqJ1k3tml7FNHa?dl=1')\n",
    "    with zipfile.ZipFile('spark_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('spark_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".appName('exmaple_app')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "정적 데이터셋의 데이터를 분석해 DataFrame을 생성함.\n",
    "이때 정적 데이터셋의 스키마도 함께 생성하며 스트림 처리과정에서 스키마를 추론하는 방법은 5장에서 자세히 알아봄.\n",
    "\n",
    "아래의 데이터는 시계열 데이터이기에 데이터를 그룹화하고 집계하는 방법을 알아볼 필요가 있음.\n",
    "이를 위해 특정 고객이 대량으로 구매하는 영업시간을 살펴보고자함.\n",
    "예를 들어 총 구매비용 컬럼을 추가하고 고객이 가장 많이 소비한 날을 찾음.\n",
    "\"\"\"\n",
    "\n",
    "staticDataFrame = spark.read.format('csv')\\\n",
    ".option('header','true')\\\n",
    ".option('inferSchema','true')\\\n",
    ".load('spark_data/retail-data/by-day/*.csv')\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView('retail_data')\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "541909\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.show(10)\n",
    "print(staticDataFrame.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+\n",
      "|CustomerID|              window|  sum(total_cost)|\n",
      "+----------+--------------------+-----------------+\n",
      "|   16057.0|[2011-12-05 09:00...|            -37.6|\n",
      "|   14126.0|[2011-11-29 09:00...|643.6300000000001|\n",
      "|   13500.0|[2011-11-16 09:00...|497.9700000000001|\n",
      "|   17160.0|[2011-11-08 09:00...|516.8499999999999|\n",
      "|   15608.0|[2011-11-11 09:00...|            122.4|\n",
      "+----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "윈도우 함수는 집계 시에 시계열 컬럼을 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우를 구성함.\n",
    "윈도우는 간격을 통해 처리 요건을 명시할 수 이기 때문에 날짜와 타임스탬프 처리에 유용함.\n",
    "스파크는 관련 날짜의 데이터를 그룹화함\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "staticDataFrame.selectExpr('CustomerID','(UnitPrice * Quantity) as total_cost','InvoiceDate')\\\n",
    ".groupby(col('CustomerID'),window(col('InvoiceDate'),'1 day'))\\\n",
    ".sum('total_cost').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "====================================================================================================\n",
      "+----------+------------------+-------------------+\n",
      "|CustomerID|        total_cost|        InvoiceDate|\n",
      "+----------+------------------+-------------------+\n",
      "|   14075.0|             85.92|2011-12-05 08:38:00|\n",
      "|   14075.0|              25.0|2011-12-05 08:38:00|\n",
      "|   14075.0|39.599999999999994|2011-12-05 08:38:00|\n",
      "|   14075.0|              30.0|2011-12-05 08:38:00|\n",
      "|   14075.0|15.299999999999999|2011-12-05 08:38:00|\n",
      "+----------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "====================================================================================================\n",
      "+----------+--------------------+-----------------+\n",
      "|CustomerID|              window|  sum(total_cost)|\n",
      "+----------+--------------------+-----------------+\n",
      "|   16057.0|[2011-12-05 09:00...|            -37.6|\n",
      "|   14126.0|[2011-11-29 09:00...|643.6300000000001|\n",
      "|   13500.0|[2011-11-16 09:00...|497.9700000000001|\n",
      "|   17160.0|[2011-11-08 09:00...|516.8499999999999|\n",
      "|   15608.0|[2011-11-11 09:00...|            122.4|\n",
      "+----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "====================================================================================================\n",
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   16705.0|[2011-12-09 09:00...|            250.52|\n",
      "|   17315.0|[2011-12-09 09:00...|              -7.5|\n",
      "|   12680.0|[2011-12-09 09:00...|249.44999999999996|\n",
      "|   15804.0|[2011-12-09 09:00...|329.04999999999995|\n",
      "|   15694.0|[2011-12-09 09:00...|             406.8|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "번외. 위 코드를 뜯어보자\n",
    "\n",
    "그리고 날짜를 바탕으로 정렬해보자\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "staticDataFrame.show(5)\n",
    "print('='*100)\n",
    "first_action = staticDataFrame.selectExpr('CustomerID','(UnitPrice * Quantity) as total_cost','InvoiceDate')\n",
    "first_action.show(5)\n",
    "## staticDataFrame에서 CustomerID와 UnitPrice * Quantity인 total_cost, InvoiceDAte 컬럼만 추출\n",
    "print('='*100)\n",
    "second_action = first_action.groupby(col('CustomerID'),window(col('InvoiceDate'),'1 day')).sum('total_cost')\n",
    "second_action.show(5)\n",
    "## 추출된 DataFrame에서 1일단위의 일자별 구매ID별 total_cost 합계를 추출\n",
    "print('='*100)\n",
    "third_action = second_action.sort(desc('window'))\n",
    "third_action.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "로컬모드로 이 코드를 실행하려면 로컬 모드에 적합한 셔플 파티션 수를 설정하는 것이 좋음.\n",
    "셔플 파티션 수는 셔플 이후에 생성될 파티션 수를 의미함. 기본값은 200이지만 로컬모드에서는 많이 필요없으므로 5로 설정\n",
    "2장에서 한번다뤘으니 참고\n",
    "\"\"\"\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.paririons','5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "위의 코드를 스트리밍 형식으로 살펴보고자함.\n",
    "코드는 거의 바뀌지않으며 read메서드 대신 readStream 메서드를 사용하는것과 maxFilesPerTrigger 옵션을 추가로 지정하는것이 차이점.\n",
    "maxFilesPerTrigger를 통해 한번에 읽을 파일수를 설정할 수 있음.\n",
    "이번 예제를 스트리밍답게 만드는 옵션이지만 운영 환경에 적용하는것은 추천하지 않음.\n",
    "\"\"\"\n",
    "\n",
    "streamingDataFrame = spark.readStream\\\n",
    ".schema(staticSchema)\\\n",
    ".option('maxFilesPerTrigger',1)\\\n",
    ".format('csv')\\\n",
    ".option('header','true')\\\n",
    ".load('spark_data/retail-data/by-day/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "만들어진 DataFrame이 스트리밍 유형인지 확인\n",
    "\"\"\"\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "기존 DataFrame처리와 동일한 비즈니스 로직을 적용.\n",
    "아래의 코드는 총 판매 금액을 계산\n",
    "이 작업 역시 지연 연산이므로 데이터 플로를 실행하기 위해 스트리밍 액션을 호출해야함\n",
    "\"\"\"\n",
    "\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr('CustomerID','(UnitPrice * Quantity) as total_cost','InvoiceDate')\\\n",
    ".groupby(col('CustomerID'),window(col('InvoiceDate'),'1 day'))\\\n",
    ".sum('total_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n스트리밍 액션은 어딘가에 데이터를 채워 넣어야하므로 count 메서드와 같은 일반적인 정적 액션과는 조금 다른 특성을 가짐.\\n여기서 사용할 스트리밍 액션은 트리거가 실행된 다음 데이터를 갱신하게 될 인메모리 테이블에 데이터를 저장함.\\n아래 코드의 경우 파일마다 트리거를 실행함. 스파크는 이전 집계값보다 더 큰 값이 발생한 경우에만 인메모리 테이블을 갱신하므로\\n언제나 가장 큰값을 얻을 수 있음\\n\\n안되서....추후 원인을 알면 수정하겠습니다...colab에선 되는데 잘모르겠네요. 제 local 경로 문제인것같습니다.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "스트리밍 액션은 어딘가에 데이터를 채워 넣어야하므로 count 메서드와 같은 일반적인 정적 액션과는 조금 다른 특성을 가짐.\n",
    "여기서 사용할 스트리밍 액션은 트리거가 실행된 다음 데이터를 갱신하게 될 인메모리 테이블에 데이터를 저장함.\n",
    "아래 코드의 경우 파일마다 트리거를 실행함. 스파크는 이전 집계값보다 더 큰 값이 발생한 경우에만 인메모리 테이블을 갱신하므로\n",
    "언제나 가장 큰값을 얻을 수 있음\n",
    "\n",
    "안되서....추후 원인을 알면 수정하겠습니다...colab에선 되는데 잘모르겠네요. 제 local 경로 문제인것같습니다.\n",
    "\"\"\"\n",
    "\n",
    "# purchaseByCustomerPerHour.writeStream\\\n",
    "# .format(\"memory\")\\\n",
    "# .queryName(\"customer_purchases\")\\\n",
    "# .outputMode(\"complete\")\\\n",
    "# .start()\n",
    "\n",
    "# spark.sql(\"\"\"SELECT * FROM customer_purchases ORDER BY 'sum(total_cost)' DESC\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "스파크는 데이터 전처리에 사용하는 다양한 메서드를 제공함. 아래 예제는 원본 데이터를 올바른 포맷으로 만드는\n",
    "트랜스포메이션을 정의하고 실제로 모델을 학습한 다음 예측을 수행\n",
    "\"\"\"\n",
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "결측치를 0으로 채우고, 요일 컬럼에 해당날짜의 요일값 추가\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "preppedDataFrame = staticDataFrame\\\n",
    ".na.fill(0)\\\n",
    ".withColumn('day_of_week',date_format(col('InvoiceDate'),'EEEE'))\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
