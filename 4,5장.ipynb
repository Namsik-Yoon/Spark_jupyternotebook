{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    findspark.init()\n",
    "except:\n",
    "    try:\n",
    "        print(os.environ['SPARK_HOME'])\n",
    "    except:\n",
    "        os.environ['SPARK_HOME'] = 'C:\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7'\n",
    "        print(os.environ['SPARK_HOME'])\n",
    "    try:\n",
    "        print(os.environ['JAVA_HOME'])\n",
    "    except KeyError:\n",
    "        os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "        print(os.environ['JAVA_HOME'])\n",
    "    print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark_data' in os.listdir():pass\n",
    "else:\n",
    "    import wget\n",
    "    import zipfile\n",
    "    wget.download('https://www.dropbox.com/sh/5c9daz43f5lstzd/AAAPN_iMkKzgqJ1k3tml7FNHa?dl=1')\n",
    "    with zipfile.ZipFile('spark_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('spark_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".appName('exmaple_app')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "스파크는 사실상 프로그래밍 언어이며 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용함.\n",
    "카탈리스트 엔진은 다양한 실행 최적화 기능을 제공하며 아래 예제는 스칼라나 파이썬이 아닌 스파크의 덧셈 연산을 수행\n",
    "\"\"\"\n",
    "\n",
    "df = spark.range(500).toDF('number')\n",
    "df.select(df['number']+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DataFrame과 Dataset 비교\n",
    "본질적으로 구조적 API에는 '비타입형'인 DataFrame과 '타입형'인 Dataset이 있음.\n",
    "\n",
    "DataFrame은 Row 타입으로 구성된 Dataset으로 Row 타입은 스파크가 사용하는 '연산에 최적화된 인메모리 포맷'의 내부적인 표현방식.\n",
    "Row 타입을 사용하면 가비지 컬렉션과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 \n",
    "매우 효율적인 연산이 가능\n",
    "\n",
    "DataFrame은 컬럼과 로우로 이루어져있으며,\n",
    "컬럼은 정수형이나 문자열 같은 단순 데이터 타입, 배열이나 맵 같은 복합 데이터 타입 그리고 null 값을 표현함.\n",
    "로우는 데이터 레코드로 SQL, RDD, 데이터소스에서 얻거나 직접 만들 수 있음. 다음은 range 메서드를 사용해 DataFrame을 생성하는 예제\n",
    "\"\"\"\n",
    "\n",
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteType"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "스파크는 여러가지 내부 데이터 타입을 가지고 있음\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "b = ByteType()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DataFrame은 Row 타입의 레코드와 각 레코드에 수행할 연산 표현식을 나타내는 여러 컬럼으로 구성됨.\n",
    "스키마는 각 컬럼명과 데이터 타입을 정의하며 DataFrame의 파티셔닝은 DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의함.\n",
    "파티셔닝 스키마는 파티션을 배치하는 방법을 정의.\n",
    "\n",
    "우선 DataFrame을 생성\n",
    "\"\"\"\n",
    "\n",
    "df = spark.read.format('json').load('spark_data/flight-data/json/2015-summary.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "스키마는 DataFrame의 컬럼명과 데이터 타입을 정의함. 데이터소스에서 스키마를 얻거나 직접 정의할 수 있음.\n",
    "\n",
    "이번 예제에서는 앞서 사용한 데이터를 사용. 이 파일은 미국 교통통계국이 제공하는 항공운항 데이터이며 줄로 구분된 반정형 json 데이터임\n",
    "스키마는 여러 개의 StructField 타입 필드로 구성된 StructType 객체이며 이름, 데이터 타입, 컬럼이 값이 없거나 null일 수 있는지\n",
    "지정하는 불리언값을 가짐.\n",
    "\"\"\"\n",
    "\n",
    "spark.read.format('json').load('spark_data/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "스키마는 복합 데이터타입인 StructType을 가질수있음. \n",
    "스파크는 런타임에 데이터 타입이 스키마의 데이터 타입과 일치하지 않으면 오류를 발생시킴\n",
    "\n",
    "metadata는 뭔지 몰겠음...\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('count', LongType(), False,metadata = {'hello':'world'})\n",
    "])\n",
    "\n",
    "df = spark.read.format('json').schema(myManualSchema).load('spark_data/flight-data/json/2015-summary.json')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
